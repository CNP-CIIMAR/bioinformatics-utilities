import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from Bio import Entrez
import requests
import speech_recognition as sr
from gtts import gTTS
import streamlit as st
from PIL import Image
import base64
import tempfile
import json
import datetime
import logging
from threading import Thread
from sentence_transformers import SentenceTransformer
import faiss
import datasets
import nltk
import numpy as np
import time
from nltk.tokenize import sent_tokenize
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple
import warnings
from requests.adapters import HTTPAdapter, Retry

# 0. Configura√ß√£o do Entrez com o e-mail para evitar erros
Entrez.email = "mattoslmp@gmail.com"
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Definir o dispositivo explicitamente como CPU
device = torch.device('cpu')

# 1. Configura√ß√£o do Streamlit
st.set_page_config(
    page_title="üêü Little Fish BOGA CIIMAR",
    page_icon="üêü",
    layout="wide",
    initial_sidebar_state="expanded",
)

# 2. Configura√ß√£o de logging
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_DIR = os.path.join(SCRIPT_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE_PATH = os.path.join(LOG_DIR, 'ai_agente.log')

logging.basicConfig(
    filename=LOG_FILE_PATH,
    filemode='a',
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

# 3. Download do NLTK
@st.cache_resource
def download_nltk_punkt():
    try:
        nltk.download('punkt', quiet=True)
        logging.info("Pacote NLTK 'punkt' baixado.")
    except Exception as e:
        logging.error(f"Erro ao baixar o pacote NLTK 'punkt': {e}")
        st.error("Ocorreu um erro ao baixar os recursos necess√°rios do NLTK.")

download_nltk_punkt()

# 4. Caminhos de Recursos
MODEL_DIR = "/mnt/disk_2TB/gpt-j-6B"  # Diret√≥rio do modelo
BACKGROUND_VIDEO_PATH = os.path.join(SCRIPT_DIR, "images", "marine_background.mp4")
PROFESSOR_IMAGE_PATH = os.path.join(SCRIPT_DIR, "images", "professor.png")
BOGA_IMAGE_PATH = os.path.join(SCRIPT_DIR, "images", "boga_image.png")
IMAGES_DIR = os.path.join(SCRIPT_DIR, "images")

# Verifica√ß√£o dos caminhos (opcional)
logging.info(f"MODEL_DIR: {MODEL_DIR}")
logging.info(f"BACKGROUND_VIDEO_PATH: {BACKGROUND_VIDEO_PATH}")
logging.info(f"PROFESSOR_IMAGE_PATH: {PROFESSOR_IMAGE_PATH}")
logging.info(f"BOGA_IMAGE_PATH: {BOGA_IMAGE_PATH}")

# 5. T√≥picos especificado
TERMS = [
    "Marine Biology", 
    "Biologia Marinha",
    "Aquaculture", 
    "Aquacultura",
    "Seafood Quality", 
    "Qualidade dos Produtos do Mar",
    "Global Changes", 
    "Mudan√ßas Globais",
    "Ecosystem Services", 
    "Servi√ßos dos Ecossistemas",
    "Marine Biotechnology", 
    "Biotecnologia Marinha",
    "Marine Environment", 
    "Ambiente Marinho",
    "Oceanography", 
    "Oceanografia",
    "Climate Change and Oceans", 
    "Mudan√ßas Clim√°ticas e Oceanos",
    "CIIMAR",
    "Cyanobacteria Natural Products", 
    "Produtos Naturais de Cianobact√©rias",
    "Cyanobacteria", 
    "Cianobact√©rias",
    "Marine Pollution", 
    "Polui√ß√£o Marinha",
    "Sustainable Fisheries", 
    "Pesca Sustent√°vel",
    "Marine Conservation", 
    "Conserva√ß√£o Marinha",
    "Marine Ecosystems", 
    "Ecossistemas Marinhos",
    "Biodiversity", 
    "Biodiversidade",
    "Marine Protected Areas", 
    "√Åreas Marinhas Protegidas",
    "Coastal Management", 
    "Gest√£o Costeira",
    "Marine Renewable Energy", 
    "Energias Renov√°veis Marinhas",
    "Blue Economy", 
    "Economia Azul"
]

# 6. Definir dispositivo para CPU
device = torch.device('cpu')
logging.info("Usando CPU")

# 7. Definir PYTORCH_CUDA_ALLOC_CONF para evitar fragmenta√ß√£o
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# 8. Fun√ß√£o para criar diret√≥rio se n√£o existir
def ensure_model_dir_exists(path):
    if not os.path.exists(path):
        os.makedirs(path)
        logging.info(f"Diret√≥rio criado: {path}")
    else:
        logging.info(f"Diret√≥rio j√° existe: {path}")

ensure_model_dir_exists(MODEL_DIR)

# 9. Fun√ß√£o para carregar o tokenizer
@st.cache_resource
def load_tokenizer(model_dir):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        logging.info("Tokenizer carregado com sucesso.")
        return tokenizer
    except Exception as e:
        logging.error(f"Erro ao carregar o tokenizer: {e}")
        st.error(f"Ocorreu um erro ao carregar o tokenizer: {e}")
        return None

# 10. Fun√ß√£o para carregar o modelo afinado
@st.cache_resource
def load_finetuned_model(model_dir):
    try:
        if not os.path.exists(model_dir):
            logging.error(f"Diret√≥rio do modelo afinado n√£o encontrado: {model_dir}")
            st.error(f"Diret√≥rio do modelo afinado n√£o encontrado: {model_dir}")
            return None
        logging.info("Carregando o modelo afinado...")
        model = AutoModelForCausalLM.from_pretrained(model_dir)
        model = model.to(device)
        logging.info("Modelo afinado carregado na CPU.")
        return model
    except Exception as e:
        logging.error(f"Erro ao carregar o modelo afinado: {e}")
        st.error(f"O modelo afinado n√£o p√¥de ser carregado. Detalhes do erro: {e}")
        return None

# 11. Fun√ß√£o para realizar o fine-tuning
def perform_finetuning(model_dir, tokenizer):
    try:
        logging.info("Iniciando o fine-tuning do modelo GPT-J-6B...")
        model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
        model.to(device)
        
        # Preparar dados
        pubmed_articles = fetch_pubmed_articles(TERMS, retmax=10000)  # Ajustado conforme necess√°rio
        wikipedia_articles = fetch_wikipedia_articles(TERMS, max_articles_per_term=1000)  # Ajustado conforme necess√°rio
        biorxiv_articles = fetch_biorxiv_articles(TERMS, max_articles=10000)  # Ajustado conforme necess√°rio
        
        # Combinar todos os artigos
        all_articles = pubmed_articles + wikipedia_articles + biorxiv_articles
        if not all_articles:
            logging.error("Nenhum artigo encontrado para os termos fornecidos durante o fine-tuning.")
            return False
        
        # Preparar os textos para o fine-tuning
        texts = [f"Title: {article['title']}\nAbstract: {article['abstract']}\n" for article in all_articles]
        
        # Criar o dataset para o fine-tuning
        dataset = datasets.Dataset.from_dict({"text": texts})
        
        # Tokenizar o dataset
        def tokenize_function(examples):
            return tokenizer(examples["text"], truncation=True, max_length=1024)
        
        tokenized_datasets = dataset.map(tokenize_function, batched=True)
        
        # Configurar argumentos de treinamento
        training_args = TrainingArguments(
            output_dir=model_dir,
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,
            learning_rate=1e-5,
            fp16=False,
            save_steps=500,
            save_total_limit=2,
            logging_steps=100,
        )
        
        # Criar Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets,
            tokenizer=tokenizer,
        )
        
        # Iniciar treinamento
        trainer.train()
        trainer.save_model(model_dir)
        logging.info("Fine-tuning conclu√≠do e modelo salvo.")
        return True
    except Exception as e:
        logging.error(f"Erro durante o fine-tuning: {e}")
        return False

# 12. Fun√ß√£o para carregar o modelo de embeddings na CPU (opcional)
@st.cache_resource
def load_embedding_model(device):
    try:
        logging.info(f"Carregando o modelo de embeddings 'all-MiniLM-L6-v2' na {device}...")
        embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')
        logging.info("Modelo de embeddings carregado.")
        return embedding_model
    except Exception as e:
        logging.error(f"Erro ao carregar o modelo de embeddings: {e}")
        st.error(f"Ocorreu um erro ao carregar o modelo de embeddings: {e}")
        return None

embedding_model = load_embedding_model(device)

# 13. Fun√ß√µes para buscar artigos

def fetch_pubmed_articles(terms, retmax=10000, years_back=5):
    """
    Busca artigos no PubMed com base nos termos fornecidos.
    """
    all_articles = []
    current_year = datetime.datetime.now().year
    mindate = f"{current_year - years_back}/01/01"
    maxdate = f"{current_year}/12/31"
    
    for term in terms:
        try:
            query = f"{term}[Title/Abstract]"
            handle = Entrez.esearch(
                db="pubmed",
                term=query,
                retmax=retmax,
                mindate=mindate,
                maxdate=maxdate,
                sort="relevance"
            )
            record = Entrez.read(handle)
            id_list = record.get("IdList", [])
            handle.close()
            time.sleep(0.34)  # Respeitar limite de taxa do NCBI
            
            if not id_list:
                logging.info(f"Nenhum artigo encontrado para o termo: {term}")
                continue
            
            # Buscar detalhes dos artigos
            handle = Entrez.efetch(db="pubmed", id=",".join(id_list), retmode="xml")
            records = Entrez.read(handle)
            handle.close()
            time.sleep(0.34)  # Respeitar limite de taxa do NCBI
            
            for record in records.get("PubmedArticle", []):
                article = record["MedlineCitation"]["Article"]
                title = article.get("ArticleTitle", "")
                abstract = article.get("Abstract", {}).get("AbstractText", "")
                if isinstance(abstract, list):
                    abstract_text = " ".join(abstract)
                else:
                    abstract_text = abstract
                all_articles.append({"title": title, "abstract": abstract_text})
            
            logging.info(f"Artigos buscados para o termo: {term}")
        
        except Exception as e:
            logging.error(f"Erro ao buscar artigos para o termo '{term}': {e}")
    
    return all_articles

def fetch_biorxiv_articles(terms, max_articles=100000):
    """
    Busca artigos no bioRxiv usando a API correta, filtrando pelo termo e p√°gina.
    """
    biorxiv_articles = []
    base_url = "https://api.biorxiv.org/details/biorxiv/"
    session = requests.Session()
    
    # Configurar retries para lidar com erros tempor√°rios
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    
    # Adicionar cabe√ßalhos para evitar bloqueios (HTTP 403)
    headers = {
        "User-Agent": "LittleFishBOGA/1.0 (mailto:mattoslmp@gmail.com)"
    }
    
    for term in terms:
        try:
            fetched = 0
            page = 0
            while fetched < max_articles:
                url = f"{base_url}{term}/{page}?format=json"
                response = session.get(url, headers=headers, timeout=10)
                
                if response.status_code == 403:
                    logging.error(f"Erro HTTP 403 ao buscar bioRxiv para o termo '{term}' na p√°gina {page}")
                    break
                elif response.status_code != 200:
                    logging.error(f"Erro HTTP {response.status_code} ao buscar bioRxiv para o termo '{term}' na p√°gina {page}")
                    break
                
                try:
                    data = response.json()
                except json.JSONDecodeError:
                    logging.error(f"Resposta vazia ou inv√°lida do bioRxiv para o termo '{term}' na p√°gina {page}")
                    break
                
                articles = data.get("collection", [])
                
                if not articles:
                    break  # Sem mais artigos
                
                for article in articles:
                    if fetched >= max_articles:
                        break
                    title = article.get("preprint_title", "")
                    abstract = article.get("abstract", "")
                    biorxiv_articles.append({"title": title, "abstract": abstract})
                    fetched += 1
                
                page += 1
                time.sleep(0.34)  # Evitar exceder limites de taxa
            
            logging.info(f"Total de {fetched} artigos buscados para o termo: {term}")
        
        except Exception as e:
            logging.error(f"Erro ao buscar artigos do bioRxiv para o termo '{term}': {e}")
    
    return biorxiv_articles

def fetch_wikipedia_articles(terms, max_articles_per_term=1000):
    """
    Busca artigos na Wikip√©dia com base nos termos fornecidos.
    """
    wikipedia_articles = []
    session = requests.Session()
    search_url = "https://en.wikipedia.org/w/api.php"
    extract_url = "https://en.wikipedia.org/w/api.php"
    
    def fetch_article_extract(page_id):
        try:
            page_params = {
                "action": "query",
                "prop": "extracts",
                "explaintext": True,
                "pageids": page_id,
                "format": "json"
            }
            page_response = session.get(url=extract_url, params=page_params, timeout=10)
            page_response.raise_for_status()
            page_data = page_response.json()
            page = page_data["query"]["pages"].get(str(page_id), {})
            title = page.get("title", "")
            extract = page.get("extract", "")
            abstract = extract[:5000]  # Limitar a 5000 caracteres
            return {"title": title, "abstract": abstract}
        except Exception as e:
            logging.error(f"Erro ao buscar extract do artigo de p√°gina ID '{page_id}': {e}")
            return None
    
    for term in terms:
        try:
            params = {
                "action": "query",
                "list": "search",
                "srsearch": term,
                "format": "json",
                "srlimit": max_articles_per_term
            }
            response = session.get(url=search_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            search_results = data.get("query", {}).get("search", [])
            
            page_ids = [result["pageid"] for result in search_results]
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_page = {executor.submit(fetch_article_extract, pid): pid for pid in page_ids}
                for future in as_completed(future_to_page):
                    result = future.result()
                    if result:
                        wikipedia_articles.append(result)
            
            logging.info(f"Total de {len(search_results)} artigos buscados para o termo: {term}")
        
        except Exception as e:
            logging.error(f"Erro ao buscar artigos da Wikip√©dia para o termo '{term}': {e}")
    
    return wikipedia_articles

# 14. Fun√ß√£o para processar e indexar artigos (opcional)
def process_and_index_articles(
    articles: List[dict],
    embedding_model: SentenceTransformer = None,
    device: torch.device = torch.device('cpu'),
    batch_size: int = 32,
    chunk_size: int = 500,
    chunk_overlap: int = 50
) -> Tuple[List[str], faiss.Index, SentenceTransformer]:
    """
    Processa e indexa artigos utilizando embeddings e FAISS.
    """
    if embedding_model is None:
        return [], None, None
    
    # Combinar t√≠tulos e resumos
    texts = [f"Title: {article.get('title', '')}\nAbstract: {article.get('abstract', '')}" for article in articles]
    logging.info(f"{len(texts)} textos combinados a partir dos artigos.")
    
    # Dividir textos em senten√ßas e ent√£o em chunks
    chunks = []
    for idx, text in enumerate(texts):
        sentences = sent_tokenize(text)
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 > chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += " " + sentence
        if current_chunk:
            chunks.append(current_chunk.strip())
        if (idx + 1) % 1000 == 0:
            logging.info(f"{idx + 1} textos processados para chunks.")
    
    logging.info(f"Total de {len(chunks)} chunks criados.")
    
    # Criar embeddings em lotes para efici√™ncia de mem√≥ria
    embeddings = []
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        try:
            batch_embeddings = embedding_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
            embeddings.append(batch_embeddings)
        except Exception as e:
            logging.error(f"Erro ao criar embeddings para o lote {i}-{i+batch_size}: {e}")
    if embeddings:
        embeddings = np.vstack(embeddings)
        logging.info("Embeddings criados.")
    else:
        embeddings = np.array([])
        logging.error("Nenhum embedding foi criado.")
    
    if embeddings.size > 0:
        # Criar o √≠ndice FAISS
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        logging.info(f"√çndice FAISS criado e {index.ntotal} vetores adicionados.")
    else:
        index = None
        logging.error("N√£o foi poss√≠vel criar o √≠ndice FAISS devido √† falta de embeddings.")
    
    return chunks, index, embedding_model

# 15. Fun√ß√£o para gerar respostas
def generate_answer(question, model, tokenizer, lang_code):
    try:
        # Define a instru√ß√£o de idioma para o modelo
        lang_instructions = {
            "pt": "Responda em Portugu√™s de forma clara e concisa.",
            "en": "Answer in English clearly and concisely.",
            "es": "Responde en Espa√±ol de forma clara y concisa."
        }
        language_instruction = lang_instructions.get(lang_code, "Answer in English clearly and concisely.")
        
        # Construir o prompt com base na pergunta e instru√ß√£o de idioma
        prompt = f"Pergunta: {question}\n{language_instruction}\nResposta:"
        
        # Tokenizar a entrada
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
        input_ids = inputs.input_ids.to(device)
        attention_mask = inputs.attention_mask.to(device)

        # Gerar a resposta usando o modelo afinado
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_length=150,  # Limitar a resposta para evitar sa√≠das muito longas
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,  # Penaliza repeti√ß√µes
                eos_token_id=tokenizer.eos_token_id
            )
        
        # Decodificar a resposta
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
        
        # Extrair a parte da resposta ap√≥s 'Resposta:'
        if "Resposta:" in answer:
            answer = answer.split("Resposta:")[-1].strip()
        elif "Answer:" in answer:
            answer = answer.split("Answer:")[-1].strip()
        elif "Respuesta:" in answer:
            answer = answer.split("Respuesta:")[-1].strip()
        
        # Verificar se a resposta n√£o est√° vazia
        if not answer:
            return "Desculpe, n√£o consegui encontrar uma resposta para sua pergunta."
        
        return answer
    except Exception as e:
        logging.error(f"Erro ao gerar a resposta: {e}")
        return "Desculpe, houve um erro ao processar sua pergunta."

# 16. Fun√ß√£o para falar o texto
def speak(text, lang_code):
    def run_tts():
        lang_speech = {
            "en": "en",
            "pt": "pt",
            "es": "es"
        }
        language = lang_speech.get(lang_code, "en")
        try:
            tts = gTTS(text=text, lang=language)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
                temp_audio = fp.name
                tts.save(temp_audio)
            st.audio(temp_audio)
            os.remove(temp_audio)
        except Exception as e:
            logging.error(f"Erro ao converter texto em fala: {e}")
    
    Thread(target=run_tts).start()

# 17. Fun√ß√£o para capturar entrada de voz
def listen(language):
    try:
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            st.write("üé§ Estou ouvindo...")
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)
            st.write("üîç Processando √°udio...")
            text = recognizer.recognize_google(audio, language=language)
            st.write(f"üó£Ô∏è Voc√™ disse: {text}")
            return text
    except sr.UnknownValueError:
        logging.error("Desculpe, n√£o entendi o que voc√™ disse.")
        st.warning("Desculpe, n√£o entendi o que voc√™ disse.")
    except sr.RequestError as e:
        logging.error(f"Erro ao se comunicar com o servi√ßo de reconhecimento de fala: {e}")
        st.error(f"Erro ao se comunicar com o servi√ßo de reconhecimento de fala: {e}")
    except sr.WaitTimeoutError:
        logging.error("Tempo de grava√ß√£o esgotado. Por favor, tente novamente.")
        st.warning("Tempo de grava√ß√£o esgotado. Por favor, tente novamente.")
    except Exception as e:
        logging.error(f"Ocorreu um erro ao capturar o √°udio: {e}")
        st.error(f"Ocorreu um erro ao capturar o √°udio: {e}")

# 18. Valida√ß√£o da pergunta
def validate_question(question):
    if not isinstance(question, str) or len(question.strip()) < 5:
        return False
    return True

# 19. Fun√ß√£o para determinar o idioma de reconhecimento de voz
def get_recognition_language(lang_code):
    lang_recognition = {
        "en": "en-US",
        "pt": "pt-BR",
        "es": "es-ES"
    }
    return lang_recognition.get(lang_code, "en-US")

# 20. Fun√ß√£o para converter imagem em base64
def get_base64_image(image_path):
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except Exception as e:
        logging.error(f"Erro ao carregar a imagem {image_path}: {e}")
        return ""

# 21. Fun√ß√£o para adicionar cor de fundo azul
def add_marine_background_color():
    marine_style = """
    <style>
    body {
        background-color: #CCE5FF;  /* Cor azul-marinha */
    }
    .stApp {
        background-color: #CCE5FF;
    }
    .footer {
        position: fixed;
        left: 0;
        bottom: 0;
        width: 100%;
        background-color: #CCE5FF;
        color: black;
        text-align: center;
        padding: 10px;
    }
    </style>
    """
    st.markdown(marine_style, unsafe_allow_html=True)

# 22. Fun√ß√£o para exibir Boga e v√≠deo lado a lado com descri√ß√µes em √°udio
def display_boga_and_video(selected_language, lang_codes):
    col1, col2 = st.columns([1, 3])

    # Descri√ß√µes das imagens em diferentes idiomas
    image_descriptions = {
        "boga_image.png": {
            "pt": "Esta √© a imagem do Boga, um peixe especial estudado na biologia marinha.",
            "en": "This is an image of Boga, a special fish studied in marine biology.",
            "es": "Esta es una imagen de Boga, un pez especial estudiado en la biolog√≠a marina."
        },
        "professor.png": {
            "pt": "Esta √© a imagem do Professor Marinho, l√≠der do CIIMAR-CNP.",
            "en": "This is an image of Professor Marinho, leader of CIIMAR-CNP.",
            "es": "Esta es una imagen del Profesor Marinho, l√≠der de CIIMAR-CNP."
        }
    }

    with col1:
        # Exibir imagem do Boga com bot√£o para descri√ß√£o em √°udio
        if os.path.exists(BOGA_IMAGE_PATH):
            boga_img_base64 = get_base64_image(BOGA_IMAGE_PATH)
            if boga_img_base64:
                boga_html = f"""
                <div style="text-align: center;">
                    <img src="data:image/png;base64,{boga_img_base64}" alt="Boga" style="max-width: 100%; height: auto;">
                    <p>Boga</p>
                </div>
                """
                st.markdown(boga_html, unsafe_allow_html=True)
                logging.info("Imagem do Boga exibida com sucesso.")
                
                # Bot√£o para ouvir descri√ß√£o
                description_pt = image_descriptions["boga_image.png"]["pt"]
                description_en = image_descriptions["boga_image.png"]["en"]
                description_es = image_descriptions["boga_image.png"]["es"]
                
                description = ""
                if selected_language == "Portugu√™s":
                    description = description_pt
                    button_label = "üîä Ouvir descri√ß√£o"
                elif selected_language == "English":
                    description = description_en
                    button_label = "üîä Listen to description"
                elif selected_language == "Espa√±ol":
                    description = description_es
                    button_label = "üîä Escuchar descripci√≥n"
                else:
                    description = description_en
                    button_label = "üîä Listen to description"
                
                if st.button(button_label + " - Boga"):
                    speak(description, lang_codes[selected_language])
            else:
                logging.error("N√£o foi poss√≠vel converter a imagem do Boga para base64.")
        else:
            logging.error(f"Arquivo de imagem do Boga n√£o encontrado: {BOGA_IMAGE_PATH}")

    with col2:
        # Exibir v√≠deo de fundo
        if os.path.exists(BACKGROUND_VIDEO_PATH):
            try:
                st.video(BACKGROUND_VIDEO_PATH)
                logging.info("V√≠deo de fundo exibido com sucesso.")
            except Exception as e:
                logging.error(f"Erro ao exibir o v√≠deo de fundo: {e}")
        else:
            logging.error(f"Arquivo de v√≠deo de fundo n√£o encontrado: {BACKGROUND_VIDEO_PATH}")

def display_professor_image(selected_language, lang_codes):
    # Descri√ß√µes das imagens em diferentes idiomas
    image_descriptions = {
        "professor.png": {
            "pt": "Esta √© a imagem do Professor Marinho, l√≠der do CIIMAR-CNP.",
            "en": "This is an image of Professor Marinho, leader of CIIMAR-CNP.",
            "es": "Esta es una imagen del Profesor Marinho, l√≠der de CIIMAR-CNP."
        }
    }

    # Exibir imagem do Professor com bot√£o para descri√ß√£o em √°udio
    if os.path.exists(PROFESSOR_IMAGE_PATH):
        professor_img_base64 = get_base64_image(PROFESSOR_IMAGE_PATH)
        if professor_img_base64:
            professor_html = f"""
            <div style="text-align: center;">
                <img src="data:image/png;base64,{professor_img_base64}" alt="Professor Marinho" style="max-width: 100%; height: auto;">
                <p>Professor Marinho</p>
            </div>
            """
            st.markdown(professor_html, unsafe_allow_html=True)
            logging.info("Imagem do Professor exibida com sucesso.")
            
            # Bot√£o para ouvir descri√ß√£o
            description_pt = image_descriptions["professor.png"]["pt"]
            description_en = image_descriptions["professor.png"]["en"]
            description_es = image_descriptions["professor.png"]["es"]
            
            description = ""
            if selected_language == "Portugu√™s":
                description = description_pt
                button_label = "üîä Ouvir descri√ß√£o"
            elif selected_language == "English":
                description = description_en
                button_label = "üîä Listen to description"
            elif selected_language == "Espa√±ol":
                description = description_es
                button_label = "üîä Escuchar descripci√≥n"
            else:
                description = description_en
                button_label = "üîä Listen to description"
            
            if st.button(button_label + " - Professor"):
                speak(description, lang_codes[selected_language])
        else:
            logging.error("N√£o foi poss√≠vel converter a imagem do Professor para base64.")
    else:
        logging.error(f"Arquivo de imagem do Professor n√£o encontrado: {PROFESSOR_IMAGE_PATH}")

# 23. Fun√ß√£o para adicionar o rodap√© com assinatura e imagem do professor
def add_footer():
    professor_img_base64 = get_base64_image(PROFESSOR_IMAGE_PATH)
    if professor_img_base64:
        footer_html = f"""
        <div class="footer">
        <div style="display: flex; justify-content: center; align-items: center;">
            <p style='color: black; margin: 0; padding-right: 10px;'>
                Authors: Leandro de Mattos Pereira<sup>1</sup>, Vitor Vasconcellos<sup>2</sup> and Pedro Le√£o<sup>1</sup> 
                <br>CIIMAR
                <br>Cyanobacterial Natural Products Laboratory<sup>1</sup>.
                <br>Blue Biotechnology, Environment and Health<sup>2</sup>
                <br>Authors and CIIMAR (Interdisciplinary Centre of Marine and Environmental Research) @2024 - All rights reserved.
            </p>
        <img src="data:image/png;base64,{professor_img_base64}" alt="Boga Fish - Professor Marinho" width="100">
            </div>
        </div>
        """   
        st.markdown(footer_html, unsafe_allow_html=True)
    else:
        logging.error("Imagem do professor n√£o p√¥de ser carregada no rodap√©.")

# 24. Fun√ß√£o principal com exibi√ß√£o de v√≠deo, imagens, rodap√© e processamento no backend
def main():
    # Adicionar fundo azul
    add_marine_background_color()

    # Sidebar de configura√ß√µes
    st.sidebar.title("Configura√ß√µes")
    show_background = st.sidebar.checkbox("Mostrar v√≠deo de fundo tem√°tico", value=False, key="show_background_checkbox")  # Desabilitado por padr√£o
    selected_language = st.sidebar.selectbox("Selecione o idioma:", ["Portugu√™s", "English", "Espa√±ol"], key="language_selectbox")
    lang_codes = {"Portugu√™s": "pt", "English": "en", "Espa√±ol": "es"}
    lang_code = lang_codes[selected_language]

    # Op√ß√£o para exibir v√≠deo de fundo adicional se necess√°rio
    if show_background and os.path.exists(BACKGROUND_VIDEO_PATH):
        st.sidebar.video(BACKGROUND_VIDEO_PATH)
    elif show_background and not os.path.exists(BACKGROUND_VIDEO_PATH):
        st.sidebar.error("Arquivo de v√≠deo de fundo n√£o encontrado.")

    # Exibir imagens com descri√ß√µes em √°udio
    col_images = st.columns(2)
    with col_images[0]:
        display_boga_and_video(selected_language, lang_codes)
    with col_images[1]:
        display_professor_image(selected_language, lang_codes)

    st.write("### Perguntas sobre biologia marinha:")
    input_method = st.radio("Escolha o m√©todo de entrada:", ("Texto", "Voz"), key="input_method_radio")

    # Carregar ou configurar o modelo
    if 'model_loaded' not in st.session_state:
        model = None
        tokenizer = None
    else:
        model = st.session_state.model
        tokenizer = st.session_state.tokenizer

    if input_method == "Texto":
        question = st.text_input("Digite sua pergunta aqui:", max_chars=500, key="question_text_input")
        if st.button("Enviar", key="send_button"):
            if question and validate_question(question):
                if 'model_loaded' not in st.session_state:
                    with st.spinner("O Boga fish est√° estudando e aprendendo - por favor, espere mais um pouco."):
                        model, tokenizer = setup_model()
                        if model and tokenizer:
                            st.session_state.model = model
                            st.session_state.tokenizer = tokenizer
                            st.session_state.model_loaded = True
                        else:
                            st.error("Falha no carregamento do modelo.")
                            st.stop()
                else:
                    model = st.session_state.model
                    tokenizer = st.session_state.tokenizer

                # Gerar a resposta
                answer = generate_answer(question, model, tokenizer, lang_code)
                st.write("#### Resposta:")
                st.write(answer)
                if answer:
                    if selected_language == "Portugu√™s":
                        button_label = "üîä Ouvir Resposta"
                    elif selected_language == "English":
                        button_label = "üîä Listen to Answer"
                    elif selected_language == "Espa√±ol":
                        button_label = "üîä Escuchar Respuesta"
                    else:
                        button_label = "üîä Listen to Answer"
                    # Bot√£o unificado para ouvir a resposta
                    if st.button(button_label, on_click=lambda: speak(answer, lang_code), key="speak_button"):
                        pass  # A a√ß√£o j√° √© realizada pelo on_click
            else:
                st.warning("Por favor, insira uma pergunta v√°lida.")
    else:
        if st.button("üé§ Gravar Pergunta", key="record_button"):
            question = listen(get_recognition_language(lang_code))
            if question:
                if validate_question(question):
                    if 'model_loaded' not in st.session_state:
                        with st.spinner("O Boga fish est√° estudando e aprendendo - por favor, espere mais um pouco."):
                            model, tokenizer = setup_model()
                            if model and tokenizer:
                                st.session_state.model = model
                                st.session_state.tokenizer = tokenizer
                                st.session_state.model_loaded = True
                            else:
                                st.error("Falha no carregamento do modelo.")
                                st.stop()
                    else:
                        model = st.session_state.model
                        tokenizer = st.session_state.tokenizer

                    # Gerar a resposta
                    answer = generate_answer(question, model, tokenizer, lang_code)
                    st.write("#### Resposta:")
                    st.write(answer)
                    if answer:
                        if selected_language == "Portugu√™s":
                            button_label = "üîä Ouvir Resposta"
                        elif selected_language == "English":
                            button_label = "üîä Listen to Answer"
                        elif selected_language == "Espa√±ol":
                            button_label = "üîä Escuchar Respuesta"
                        else:
                            button_label = "üîä Listen to Answer"
                        # Bot√£o unificado para ouvir a resposta
                        if st.button(button_label, on_click=lambda: speak(answer, lang_code), key="speak_button"):
                            pass  # A a√ß√£o j√° √© realizada pelo on_click
                else:
                    st.warning("Por favor, insira uma pergunta v√°lida.")
            else:
                st.warning("N√£o foi poss√≠vel obter sua pergunta.")

    # Adicionar rodap√© com assinatura e imagem do professor
    add_footer()

# 25. Fun√ß√£o para carregar o modelo ou realizar fine-tuning se necess√°rio
def setup_model():
    if not os.listdir(MODEL_DIR):  # Verifica se o diret√≥rio est√° vazio
        logging.info("Modelo afinado n√£o encontrado. Iniciando o fine-tuning.")
        tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
        success = perform_finetuning(MODEL_DIR, tokenizer)
        if success:
            # Recarregar o tokenizer e o modelo afinado ap√≥s o fine-tuning
            tokenizer = load_tokenizer(MODEL_DIR)
            model = load_finetuned_model(MODEL_DIR)
            return model, tokenizer
        else:
            logging.error("Falha no fine-tuning do modelo.")
            return None, None
    else:
        logging.info("Modelo afinado encontrado. Carregando o modelo...")
        tokenizer = load_tokenizer(MODEL_DIR)
        model = load_finetuned_model(MODEL_DIR)
        return model, tokenizer

if __name__ == "__main__":
    # Suprimir warnings espec√≠ficos
    warnings.filterwarnings("ignore", category=FutureWarning, module="transformers.tokenization_utils_base")
    warnings.filterwarnings("ignore", category=UserWarning, module="torch._utils")
    main()

